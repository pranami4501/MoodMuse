{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X32pvUypVNjf",
        "outputId": "e686cbef-8b0c-4daf-bfa2-c6b260e8d24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgS9PHIUWD_E",
        "outputId": "4eefb8f3-d22f-4956-bdb2-d0f298f0ae87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 | Platform: x86_64\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA: False\n"
          ]
        }
      ],
      "source": [
        "import torch, platform, sys\n",
        "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.machine())\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOpFkcexWxqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2671ba-4d8f-4787-9433-11cc18eb0bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U transformers datasets evaluate rouge-score bert-score sentencepiece accelerate tiktoken einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eVZyTM0Yn5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8fd5fd-3686-462e-ee5f-a74932c6c281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langdetect ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNIPZfTaYoSY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p moodmuse/{data,models,src}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTog95fZG9A",
        "outputId": "a38d85bd-7a4a-4368-eb3a-6a8c0cae6369"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'root': '/content/MoodMuse',\n",
              " 'raw_dir': '/content/MoodMuse/data/raw',\n",
              " 'proc_dir': '/content/MoodMuse/data/processed',\n",
              " 'ckpt_dir': '/content/MoodMuse/outputs/checkpoints',\n",
              " 'samples_dir': '/content/MoodMuse/outputs/samples'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/content/MoodMuse\")\n",
        "for p in [\n",
        "    ROOT/\"data/raw\",\n",
        "    ROOT/\"data/processed\",\n",
        "    ROOT/\"outputs/checkpoints\",\n",
        "    ROOT/\"outputs/samples\",\n",
        "    ROOT/\"src\",\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# quick config helper we'll reuse\n",
        "CONFIG = {\n",
        "    \"root\": str(ROOT),\n",
        "    \"raw_dir\": str(ROOT/\"data/raw\"),\n",
        "    \"proc_dir\": str(ROOT/\"data/processed\"),\n",
        "    \"ckpt_dir\": str(ROOT/\"outputs/checkpoints\"),\n",
        "    \"samples_dir\": str(ROOT/\"outputs/samples\"),\n",
        "}\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "sIxJp_EfZQRY",
        "outputId": "3210152b-7066-48a1-f30b-1f6d025ed4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬆️ Upload kaggle.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5e64ca78-a64e-4e0d-807b-2bb8db48ca7f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5e64ca78-a64e-4e0d-807b-2bb8db48ca7f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "import os, json, pathlib\n",
        "from google.colab import files\n",
        "\n",
        "kaggle_dir = pathlib.Path(\"/root/.kaggle\")\n",
        "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not (kaggle_dir/\"kaggle.json\").exists():\n",
        "    print(\"⬆️ Upload kaggle.json\")\n",
        "    uploaded = files.upload()\n",
        "    with open(kaggle_dir/\"kaggle.json\", \"wb\") as f:\n",
        "        f.write(uploaded[\"kaggle.json\"])\n",
        "    os.chmod(kaggle_dir/\"kaggle.json\", 0o600)\n",
        "\n",
        "!kaggle --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFUtVz66aoGn",
        "outputId": "e41b0cd3-e171-4067-dba5-05c3feeeac12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle API 1.7.4.5\n",
            "-rw------- 1 root root 69 Sep  2 16:00 /root/.kaggle/kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!kaggle --version\n",
        "!ls -l /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvzUcX8_avcp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lysHFQca-DZ",
        "outputId": "91fb4a33-fc1f-4544-cb29-9e67f88ea763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref,title,size,lastUpdated,downloadCount,voteCount,usabilityRating\r\n",
            "prithvijaunjale/instagram-images-with-captions,Instagram Images with Captions,4120862937,2020-03-21 11:34:12.083000,2772,59,0.4117647\r\n",
            "propriyam/instagram-data,Instagram Data,3349500,2024-03-14 06:41:29.870000,1908,33,1.0\r\n",
            "thecoderenroute/instagram-posts-dataset,Instagram posts dataset,1815072617,2023-05-09 11:51:56.433000,1251,18,0.875\r\n",
            "rxsraghavagrawal/instagram-reach,Instagram Reach,17260,2021-01-21 13:34:20.880000,2585,15,0.47058824\r\n",
            "faiznfi/surabaa-high-school-instagram-caption,Surabaya High School Instagram Caption,137314,2018-08-13 02:28:28.227000,83,2,0.5882353\r\n",
            "amankumarkharwal/instagram-reach-data,Instagram Reach Data,11201,2022-04-07 17:04:25.450000,258,6,0.47058824\r\n",
            "krpurba/fakeauthentic-user-instagram,Fake/Authentic User Instagram,3495878,2023-11-05 01:06:16.493000,1034,19,0.7058824\r\n",
            "rezaunderfit/what-people-are-talking-about-pfizer-on-instagram,Pfizer Instagram posts,250793,2021-01-19 07:03:51.647000,106,5,0.7647059\r\n",
            "sarthakkapaliya/instagram-caption-data,Instagram Caption Data,478326,2025-04-21 00:38:20.763000,15,0,0.4117647\r\n",
            "dumbergerl/instagram-posts-of-macroinfluencers-2019,Instagram Posts of Macro-Influencers (2019),46870,2022-06-14 07:15:47.737000,249,12,0.6875\r\n",
            "alfianhakim/mbti-kpop-instagram-captions,Instagram Captions of K-Pop Idols with Their MBTI,5710412,2023-01-18 19:46:29.133000,177,1,0.5294118\r\n",
            "ercharugoyal/instagram-reach-analysis-using-python,Instagram Reach Analysis using Python,11201,2023-09-03 08:16:40.293000,92,1,0.47058824\r\n",
            "saurabhprakashgiri/instagram-post-reach,Instagram Post Reach,17260,2021-01-23 03:58:46.470000,451,8,0.5882353\r\n",
            "azrilfahmiardi/instagram-influencer-and-brand,Instagram Influencer & Brand,643081,2025-07-09 16:53:26.877000,18,0,0.5882353\r\n",
            "raghadalmarshadi/instagram-reach-analysis-excel-project,Instagram Reach Analysis - Excel Project,291841,2025-06-14 03:26:20.040000,10,0,0.5294118\r\n",
            "omenkj/social-media-sponsorship-and-engagement-dataset,Social Media Sponsorship & Engagement Dataset,8047768,2025-05-28 14:00:12.423000,92,0,0.5294118\r\n",
            "waleedgamaa/instagram-reach-analysis,Instagram Analysis with all regressions algorithms,11215,2023-08-06 20:44:57.807000,79,4,0.5294118\r\n",
            "ppprabbit/indo-instagram-posts,indo_instagram_posts,700845856,2021-10-01 06:46:48.407000,24,1,0.11764706\r\n",
            "imhits/instagram-scraper-apis-location-post,Instagram Scraper API's Location Post,131414,2024-10-08 16:20:11.677000,11,0,0.23529412\r\n",
            "sumukhkesarla/instagram-posts-with-cybersecurity,Instagram posts with #cybersecurity,12637444,2023-01-31 10:31:54.947000,34,0,0.11764706\n",
            "ref,title,size,lastUpdated,downloadCount,voteCount,usabilityRating\n",
            "sanjanchaudhari/user-behavior-on-instagram,User Behavior on Instagram📱🤳🙎‍♂️,114736,2023-07-20 09:01:52.413000,2466,44,0.7647059\n",
            "dumbergerl/instagram-posts-of-macroinfluencers-2019,Instagram Posts of Macro-Influencers (2019),46870,2022-06-14 07:15:47.737000,249,12,0.6875\n",
            "thedevastator/web-harvested-image-and-caption-dataset,Web-Harvested Image and Caption Dataset,233254845,2023-12-06 01:44:40.967000,181,7,1.0\n",
            "thedevastator/jojo-siwa-s-twitter-insights-likes-media-engagem,Jojo Siwa's Tweets,459923,2022-12-21 03:42:17.370000,54,1,0.9411765\n",
            "krpurba/fakeauthentic-user-instagram,Fake/Authentic User Instagram,3495878,2023-11-05 01:06:16.493000,1034,19,0.7058824\n",
            "tariwetter/simplified-workflow-transform-audio-into-stunning,Simplified Workflow Transform Audio into Stunning,260149,2025-02-26 09:53:12.230000,2,0,0.5625\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list -s \"instagram caption\" -v\n",
        "!kaggle datasets list -s \"social media captions\" -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkcDUaA-bs7V",
        "outputId": "13c88ebf-b6d3-4749-b513-35d406df9d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/propriyam/instagram-data\n",
            "License(s): MIT\n",
            "Downloading instagram-data.zip to data\n",
            "  0% 0.00/3.19M [00:00<?, ?B/s]\n",
            "100% 3.19M/3.19M [00:00<00:00, 852MB/s]\n",
            "data:\n",
            "instagram_data.csv\n"
          ]
        }
      ],
      "source": [
        "DATASET = \"propriyam/instagram-data\"\n",
        "!kaggle datasets download -d $DATASET -p data --unzip\n",
        "!ls -R data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gVm95JocVb6"
      },
      "source": [
        "Loading and peeking into the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7mnM6I6cT9B",
        "outputId": "c48f014f-275f-4896-f627-314bb1263308"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11692, 14),\n",
              " ['owner_id',\n",
              "  'owner_username',\n",
              "  'shortcode',\n",
              "  'is_video',\n",
              "  'caption',\n",
              "  'comments',\n",
              "  'likes',\n",
              "  'created_at',\n",
              "  'location',\n",
              "  'imageUrl',\n",
              "  'multiple_images',\n",
              "  'username',\n",
              "  'followers',\n",
              "  'following'],\n",
              "    owner_id     owner_username    shortcode is_video  \\\n",
              " 0  36063641  christendominique  C3_GS1ASeWI    False   \n",
              " 1  36063641  christendominique  C38ivgNS3IX     True   \n",
              " 2  36063641  christendominique  C35-Dd9SO1b     True   \n",
              " \n",
              "                                              caption comments  likes  \\\n",
              " 0  I’m a brunch & Iced Coffee girlie☕️🍳 \\n\\nTop @...      268  16382   \n",
              " 1  😮‍💨Brow tips I really wish I would have know w...      138   9267   \n",
              " 2  OMG I can’t believe it’s already been 1 yr sin...     1089  10100   \n",
              " \n",
              "      created_at location                                           imageUrl  \\\n",
              " 0  1.709327e+09      NaN  https://instagram.flba2-1.fna.fbcdn.net/v/t39....   \n",
              " 1  1.709241e+09      NaN  https://instagram.flba2-1.fna.fbcdn.net/v/t51....   \n",
              " 2  1.709155e+09      NaN  https://instagram.flba2-1.fna.fbcdn.net/v/t51....   \n",
              " \n",
              "   multiple_images           username  followers  following  \n",
              " 0            True  christendominique  2144626.0     1021.0  \n",
              " 1           False  christendominique  2144626.0     1021.0  \n",
              " 2           False  christendominique  2144626.0     1021.0  )"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"data/instagram_data.csv\"\n",
        "df = pd.read_csv(path, low_memory=False)\n",
        "\n",
        "df.shape, df.columns.tolist(), df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df85AIqFc37z",
        "outputId": "d5d2c8bb-60f0-4d81-9c46-af839763015a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['caption']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# basic caption diagnostics\n",
        "cap_col_candidates = [c for c in df.columns if 'caption' in c.lower() or 'text' in c.lower()]\n",
        "cap_col_candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ZGaNNrc52F",
        "outputId": "c00132e3-6638-441e-f384-6717e0dc6a5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.013770099213137188),\n",
              " np.float64(0.31388983920629493),\n",
              " count    11531.000000\n",
              " mean       311.179256\n",
              " std        396.987874\n",
              " min          1.000000\n",
              " 25%         55.000000\n",
              " 50%        155.000000\n",
              " 75%        402.000000\n",
              " max       2200.000000\n",
              " Name: caption, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "cap_col = cap_col_candidates[0]  # picking the first match for now\n",
        "df[cap_col].isna().mean(), df[cap_col].duplicated().mean(), df[cap_col].str.len().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6x1C601c9_o",
        "outputId": "ca1423c9-3a78-4606-f5ee-589d9c32bf70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Akhirnya buka juga @tokokopituku 😭🙌\\n.\\n#tokokopituku #eskopibandung #makanpakereceh #bandungfoodies #kulinerbandung #wisatabandung',\n",
              " 'Creamy tomato burrata pasta 🍅\\n\\nThis pasta dish is such a great, simple meal for the Summertime! It’s super easy to make and absolutely delicious, hope you enjoy 😋\\n\\n(Serves two)\\n\\nIngredients:\\u2028- 2 tbsp butter\\u2028- 1 garlic clove, minced\\u2028- 1 whole chilli, diced\\u2028- 250g cherry tomatoes\\u2028- 2 tbsp tomato paste\\u2028- 1 cup thickened cream\\u2028- 250g pasta, cooked al dente\\u2028- 150g burrata\\u2028- fresh basil leaves, to serve\\u2028- salt and pepper, to taste\\n\\nMethod:\\u20281. Cook your pasta and prepare garlic and chilli\\u20282. Place a large frying pan over medium-high heat on the stove, add butter and melt\\u20283. Add garlic, chilli and cherry tomatoes and fry for 3-4 minutes\\u20284. Add tomato paste, salt and pepper and continue frying for a further 3 minutes\\u20285. Add thickened cream and reduce heat to low-medium. Continue cooking, stirring occasionally, for a further 6-7 minutes or until the cherry tomatoes begin to break apart easily\\u20286. Add pre-cooked pasta and mix well with the sauce\\u20287. Tear burrata into chunks and pick fresh basil leaves. Add to the pasta\\u20288. Enjoy!\\n\\n#burrata #burratarecipe #pastarecipe #summer #pasta #easymeals',\n",
              " 'when the bass & keys player don’t get along.\\n\\n🚀 introducing @pickupbass - a community dedicated to all things bass.\\n\\n👉 if you want to learn bass, today we launched our Bass for Guitarists Learning Pathway. It’s a 3 month online guided program for guitarists.\\n\\nShout out to the musicians! @corbinjonesbass @ramumusic @garrit_tillman @karl_kerfoot @friendshipwizard @nickcampbelldestroys \\n#bass #bassguitar #pickupbass',\n",
              " 'Weekend escape with the all new @mercedesbenzitalia amg C63 s ❄️\\nsupplied by #mercedes  #amgc63s #amg #4matic #snow #winter',\n",
              " 'Migratory interests. I’ve been taking a little break from pixel painting and posting,  exploring a few new fascinations.  Then last month, DJI asked me to try out their Mavic3. Always happy to see through a different lens, I spent a couple weeks dodging fall storms in Idaho. (Really digging the extended battery life BTW.) Ironically, I’m now realizing my artistic process; harmonizing an eclectic array of images (somewhere around 30 here), isn’t so different than life process; harmonizing an eclectic array of interests. I really think the key is remembering to identify with the fascination, not the subject of it. Trust that single source of light, and the rest will sort itself in time, or Photoshop.\\n\\n@DJIglobal #mavic3 #imageaboveeverything',\n",
              " 'Outfits of the week - which one do you choose? 🙌🏽',\n",
              " 'Saturday 🐰🐇🐴',\n",
              " 'It’s the off shoulder season ☕️ @dior\\nPhoto @jeffthibodeauco \\nVideo @thestyleograph \\n#dioraw24',\n",
              " '小鳳…. 不用怕！萬大事有玲瓏姐在～\\n我要替你報仇雪恨，不能讓人欺負你🥺\\n\\n@dadadawongx \\n\\n#狀王之王 #玲瓏 #小鳳 \\n#姊妹情 #使節姦殺案 \\n#萬大事情有我在',\n",
              " 'New road in Talia, for @nytmag #archive']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df[cap_col].dropna().sample(10, random_state=42).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCCCgcVzdsAa"
      },
      "source": [
        "Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGAt9yqXdjJ7",
        "outputId": "81d1db04-7b19-4d20-9e22-917fbd1516f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final size: 7906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Swimwear that matches the ocean 💙 @monycaeleogram wearing our most supportive full piece Rhea in Lapis Terry Ribbed 🌞 \\nVideo by @sierrasavannahf \\n.\\n#super8 #swimwear #super8film #swimwearfashion #hawaiistagram',\n",
              " 'Desliza para saltar! Rétanos acá abajo estaremos cumpliendo los que tengan más likes',\n",
              " 'Paris dump 📸\\n難得在時裝周的一日假期\\n雖然也用了半天的時間拍攝工作\\n但今早起床也有叉過電的感覺💆🏻\\u200d♀️\\n下午就是期待已久的valentino show～時尚快遞很快繼續更新🤘🏻\\n#elvainpfw\\n#orsaymuseum',\n",
              " 'new beginnings 🏋🏼\\u200d♀️ @crossfitlevanger \\n#crossfitlevanger #crossfit #levanger #sprek',\n",
              " 'Keeping it classy for the @hermes show',\n",
              " 'Anzeige // thank you for having me @rabanne 🤩 (PSA: I did not wake up like this) #rabanneparfums',\n",
              " 'Walks around #Wapping 🌚',\n",
              " 'mountain ⛷️ time',\n",
              " 'New York’a iner inmez ayağımın tozuyla  @burak.piri.araz ‘ı alıp doğru Central Park’ın yolunu tuttum. Metrodan Time Square’de inip başladık yürümeye ilk durağımız tabii ki İtalyan pizzacısı kaptık ikişer dilim pizzayı🍕🍕yola devam…\\n\\nHava mı?Hava buzzz 1-2 derecelerde. ❄️ Burak 20 gündür New York’ta muhtar gibi anlatıyor her yeri. New York’a 4 yıldır uğramıyordum ne değişmiş… Hiçbir şey! Neyse kalabalığı yarıp yürümeye devam, pizzaları gömdük keyfimiz yerinde. ☺️\\n\\nBir süre sonra parka giriş yaptık her köşede bir sincap 🐿️. Mevsimden dolayı parkın renkleri çok güzel ama bir hafta önceki renkleri bir de Burak’tan dinleyin. Başladık hava iyiden iyiye buz kesmeye başladı hele güneşin görünmediği kısımlar daha fena. 🥶\\n\\nBirazda fotoğraf çekmenin zamanı geldi diyerekkkk aldım telefonu elime. Uygun açıları aramaya başladım. 📸 \\n\\nSevdiğim fotoğraflar çıktı ve 75. Amerika turuna başlamış oldum. 🚗\\n\\n📍İlk durak New York! Yarın mı? Bekle bizi New Mexico!✈️\\n#newyork #autmn #usa #olayseven #centralpark',\n",
              " 'Together, we’re unstoppable 🫶 \\n \\nYou’re busy. We get it. But we have something in common: we love to move. \\n \\nThere’s nothing we love more than getting together with our sisterhood. So, as the pace of life picks back up, we’re finding ways to gather the girls for a moment of mindfulness. Because making time to move together is the best way to recharge and reconnect. \\n \\nTag someone you love to move with. \\n \\n#dontsweatit #iamasweatybetty #rechargeandreconnect #sisterhood']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "captions = df[\"caption\"].dropna().drop_duplicates().str.strip()\n",
        "\n",
        "# Removing very short captions (the ones maybe less 5 chars)\n",
        "\n",
        "captions = captions[captions.str.len() >= 5]\n",
        "\n",
        "# Restting the index\n",
        "\n",
        "captions = captions.reset_index(drop=True)\n",
        "\n",
        "print(\"Final size:\", len(captions))\n",
        "captions.sample(10, random_state=42).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkhWIvq_facT",
        "outputId": "620a962b-3950-4c7e-d66b-93d4f3cd79aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 7906 captions to data/captions_clean.txt\n"
          ]
        }
      ],
      "source": [
        "out_path = \"data/captions_clean.txt\"\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for cap in captions:\n",
        "        f.write(cap + \"\\n\")\n",
        "\n",
        "print(f\"Saved {len(captions)} captions to {out_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ-6CUaiZQ8W"
      },
      "source": [
        "Filtering cleaned captions to English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9clicRbZFJ9",
        "outputId": "03fc3065-5f69-4eaf-a7b8-d13a7a9c5826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept 16594 / 28622 captions as English-only → data/captions_en.txt\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect_langs, DetectorFactory\n",
        "from pathlib import Path\n",
        "import re\n",
        "from ftfy import fix_text\n",
        "\n",
        "DetectorFactory.seed = 0  # make langdetect deterministic\n",
        "\n",
        "src_path = Path(\"data/captions_clean.txt\")\n",
        "lines = [ln.strip() for ln in src_path.read_text(encoding=\"utf-8\").splitlines()]\n",
        "lines = [ln for ln in lines if ln]  # drop empties\n",
        "\n",
        "url_pat = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "handle_pat = re.compile(r\"@\\w+\")\n",
        "hash_pat = re.compile(r\"#\\w+\")\n",
        "emoji_pat = re.compile(\n",
        "    \"[\\U0001F300-\\U0001F6FF\\U0001F900-\\U0001F9FF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]\"\n",
        ")\n",
        "\n",
        "def ascii_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    a = sum(1 for ch in s if ord(ch) < 128)\n",
        "    return a / len(s)\n",
        "\n",
        "def is_english(s: str) -> bool:\n",
        "    # light cleaning for detection only (keeping original text)\n",
        "    probe = url_pat.sub(\" \", s)\n",
        "    probe = handle_pat.sub(\" \", probe)\n",
        "    probe = hash_pat.sub(\" \", probe)\n",
        "    probe = emoji_pat.sub(\" \", probe)\n",
        "    probe = fix_text(probe).strip()\n",
        "\n",
        "    # quick heuristic guards (helps the detector)\n",
        "    if ascii_ratio(probe) < 0.7:\n",
        "        return False\n",
        "    if len(probe) < 12:\n",
        "        # tiny snippets confuse detectors; allow short ASCII-ish lines\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        langs = detect_langs(probe)\n",
        "        # keeping if English has decent probability\n",
        "        for lp in langs:\n",
        "            if lp.lang == \"en\" and lp.prob >= 0.80:\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        # fallback on heuristic if detector fails\n",
        "        return ascii_ratio(probe) > 0.85\n",
        "\n",
        "english_lines = [ln for ln in lines if is_english(ln)]\n",
        "\n",
        "out_path = Path(\"data/captions_en.txt\")\n",
        "out_path.write_text(\"\\n\".join(english_lines), encoding=\"utf-8\")\n",
        "print(f\"Kept {len(english_lines)} / {len(lines)} captions as English-only → {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbWe3VFThaM8"
      },
      "source": [
        "Splitting into train / validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svw4xoX8hK2F",
        "outputId": "e85f096d-b7d9-4d73-8f92-c2f59ee17a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total EN: 16594 | train: 14935 | val: 1659\n",
            "train sample -> Steadicam @distefano\n",
            "val   sample -> Why did the sugar story not publish?\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "src = Path(\"data/captions_en.txt\")   # << using the English-only file\n",
        "lines = [ln.rstrip(\"\\n\") for ln in src.read_text(encoding=\"utf-8\").splitlines()]\n",
        "lines = [ln for ln in lines if ln.strip()]\n",
        "\n",
        "random.shuffle(lines)\n",
        "\n",
        "n = len(lines)\n",
        "n_val = max(1, int(0.10 * n))\n",
        "val_lines = lines[:n_val]\n",
        "train_lines = lines[n_val:]\n",
        "\n",
        "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"data/train.txt\").write_text(\"\\n\".join(train_lines), encoding=\"utf-8\")\n",
        "Path(\"data/val.txt\").write_text(\"\\n\".join(val_lines), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Total EN: {n} | train: {len(train_lines)} | val: {len(val_lines)}\")\n",
        "print(\"train sample ->\", train_lines[0][:120])\n",
        "print(\"val   sample ->\", val_lines[0][:120])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So39CU67DMUp",
        "outputId": "00c9080f-405b-4e21-c500-aef851a1f668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 1154\n",
            "Sample encode/decode check:\n",
            "[79, 81, 81, 70, 2, 79, 87, 85, 71] -> mood muse\n",
            "Saved tokenizer to data/tokenizer_char.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "train_text = Path(\"data/train.txt\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "# Building vocabulary from TRAIN only (avoid val leakage)\n",
        "\n",
        "chars = sorted(list(set(train_text)))\n",
        "itos = {i:c for i,c in enumerate(chars)}\n",
        "stoi = {c:i for i,c in itos.items()}\n",
        "\n",
        "def encode(s: str):\n",
        "  return [stoi[c] for c in s if c in stoi]\n",
        "\n",
        "def decode(ids):\n",
        "  return \"\".join(itos[i] for i in ids)\n",
        "\n",
        "print(\"Vocab size:\", len(chars))\n",
        "print(\"Sample encode/decode check:\")\n",
        "test = \"mood muse\"\n",
        "print(encode(test)[:20], \"->\", decode(encode(test)))\n",
        "\n",
        "\n",
        "tok_path = Path(\"data/tokenizer_char.json\")\n",
        "tok_path.write_text(json.dumps({\"stoi\": stoi, \"itos\": itos}, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(\"Saved tokenizer to\", tok_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8ia8GhFHhs6",
        "outputId": "dd3ee1e1-2ace-4afb-c879-58b064c223fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg tokens: 107.4 | max: 1278 | min: 2\n"
          ]
        }
      ],
      "source": [
        "# token-length distribution on a small slice (for speed)\n",
        "sampled = train_lines[:1000]\n",
        "lengths = [len(encode(x+\"\\n\")) for x in sampled]  # include newline like classic text corpora\n",
        "print(f\"Avg tokens: {sum(lengths)/len(lengths):.1f} | max: {max(lengths)} | min: {min(lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT7t9IxBISfp",
        "outputId": "ed074621-5657-4d79-a648-d83c637879f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train tokens: 1599397  | val tokens: 167511  | vocab: 1154\n"
          ]
        }
      ],
      "source": [
        "import torch, math\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# load tokenizer\n",
        "\n",
        "tok = json.loads(Path(\"data/tokenizer_char.json\").read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# Ensure stoi keys are strings and values are integers\n",
        "stoi = {k: int(v) for k, v in tok[\"stoi\"].items()}\n",
        "\n",
        "# Ensure itos keys are integers and values are strings\n",
        "itos = {int(k): v for k, v in tok[\"itos\"].items()}\n",
        "\n",
        "\n",
        "def encode(s): return torch.tensor([stoi[c] for c in s if c in stoi], dtype=torch.long)\n",
        "def decode(ids): return \"\".join(itos[int(i)] for i in ids) # Added .item() to extract scalar from tensor\n",
        "\n",
        "\n",
        "# Read data\n",
        "\n",
        "train_text = Path(\"data/train.txt\").read_text(encoding=\"utf-8\")\n",
        "val_text = Path(\"data/val.txt\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "# Simple separator to help the model see caption boundaries\n",
        "\n",
        "SEP = \"\\n\"\n",
        "train_ids = encode(train_text + SEP)\n",
        "val_ids = encode(val_text + SEP)\n",
        "\n",
        "vocab_size = len(itos)\n",
        "print(\"train tokens:\", len(train_ids), \" | val tokens:\", len(val_ids), \" | vocab:\", vocab_size)\n",
        "\n",
        "# Batching utils\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "block_size = 128 # Corrected black_size to block_size based on usage below\n",
        "batch_size = 64\n",
        "\n",
        "def get_batch(split): # Corrected splits to split based on usage below\n",
        "  data = train_ids if split == \"train\" else val_ids\n",
        "  ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UPmGKi0OhW-"
      },
      "source": [
        "Defining the model and running a short training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3cH3j-kOgTo",
        "outputId": "ab1f2a2d-de9e-4418-e39b-227bf8c37484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step  100 | train 2.735 | val 2.763\n",
            "step  200 | train 2.658 | val 2.679\n",
            "step  300 | train 2.610 | val 2.659\n",
            "step  400 | train 2.535 | val 2.580\n",
            "step  500 | train 2.456 | val 2.511\n",
            "step  600 | train 2.352 | val 2.378\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "n_embd   = 256\n",
        "n_head   = 4\n",
        "n_layer  = 4\n",
        "dropout  = 0.2\n",
        "lr       = 3e-4\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "        self.qkv = nn.Linear(n_embd, 3*n_embd, bias=False)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.resid_drop = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.size()\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1)\n",
        "        q = q.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
        "        k = k.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
        "        v = v.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
        "        att = (q @ k.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout)\n",
        "        self.mlp  = MLP(n_embd, dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, dropout):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init)\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "        return idx\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        tok = self.tok_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "        x = self.drop(tok + pos)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
        "        return logits, loss\n",
        "\n",
        "model = TinyGPT(vocab_size, n_embd, n_head, n_layer, dropout).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.01)\n",
        "\n",
        "max_steps = 600\n",
        "log_every = 100\n",
        "\n",
        "model.train()\n",
        "for step in range(1, max_steps+1):\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    _, loss = model(xb, yb)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    if step % log_every == 0:\n",
        "        with torch.no_grad():\n",
        "            xbv, ybv = get_batch(\"val\")\n",
        "            _, vloss = model(xbv, ybv)\n",
        "        print(f\"step {step:4d} | train {loss.item():.3f} | val {vloss.item():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"STYLE: dreamy minimal | SCENE: golden hour at the beach | MOOD: soft & warm | CAPTION:\""
      ],
      "metadata": {
        "id": "gu8tqHwBL32R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_text(prompt=\"\", max_new_tokens=120, temperature=0.8, top_k=None, top_p=0.9,\n",
        "                rep_penalty=1.12, n_samples=5):\n",
        "    model.eval()\n",
        "    start = encode(prompt if prompt else \"\\n\").unsqueeze(0).to(device)\n",
        "    out = []\n",
        "    for _ in range(n_samples):\n",
        "        idx = start.clone()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "\n",
        "            # repetition penalty (simple token-level)\n",
        "            if rep_penalty != 1.0:\n",
        "                seen = torch.bincount(idx.view(-1), minlength=vocab_size).clamp(max=1).float()\n",
        "                logits = logits - (rep_penalty - 1.0) * seen.unsqueeze(0).to(logits.device)\n",
        "\n",
        "            # top-k\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
        "\n",
        "            # nucleus (top-p)\n",
        "            if top_p is not None:\n",
        "                sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
        "                cumprobs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                mask = cumprobs > top_p\n",
        "                mask[..., 1:] = mask[..., :-1].clone()\n",
        "                mask[..., 0] = False\n",
        "                sorted_logits[mask] = -float(\"inf\")\n",
        "                logits = torch.zeros_like(logits).scatter_(1, sorted_idx, sorted_logits)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "            if int(next_id) == stoi.get(\"\\n\", -999):\n",
        "                break\n",
        "        out.append(decode(idx[0].tolist()))\n",
        "    return out"
      ],
      "metadata": {
        "id": "A9pMbbI7LnWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def polish(s: str):\n",
        "    s = re.sub(r'[ \\t]+', ' ', s).strip()\n",
        "    s = re.sub(r'([.!?])\\1{2,}', r'\\1\\1', s)\n",
        "    s = re.sub(r'([#@]\\S+)(?:\\s+\\1)+', r'\\1', s)  # dedupe tags/mentions\n",
        "    tags = re.findall(r'#\\w+', s)\n",
        "    if len(tags) > 3:\n",
        "        keep = tags[:3]\n",
        "        s = re.sub(r'#\\w+', '', s)\n",
        "        s = s.strip() + \" \" + \" \".join(keep)\n",
        "    return s.strip()\n"
      ],
      "metadata": {
        "id": "8HhcI1MdL_Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL8tpT6YZGQQ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_text(prompt=\"\", n_samples=3, max_new_tokens=120, temperature=0.8, top_k=60):\n",
        "    model.eval()\n",
        "    start = encode(prompt if prompt else \"\\n\").unsqueeze(0).to(device)\n",
        "    out = []\n",
        "    for _ in range(n_samples):\n",
        "        idx = start.clone()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "            if int(next_id) == stoi.get(\"\\n\", -999):\n",
        "                break\n",
        "        out.append(decode(idx[0].tolist()))\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def decode_ascii(ids):\n",
        "    # Allowing printable ASCII + newline; mapping everything else to space\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        ch = itos[int(i)]\n",
        "        o = ord(ch)\n",
        "        if ch == \"\\n\" or (32 <= o <= 126):\n",
        "            out.append(ch)\n",
        "        else:\n",
        "            out.append(\" \")\n",
        "    txt = \"\".join(out)\n",
        "    # collapsing weird spaces\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "def tidy_caption(s, max_len=140):\n",
        "    # keeping hashtags/@, punctuation; stripping anything non-ASCII that slipped\n",
        "    s = s.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"[^A-Za-z0-9#@&.,!?'\\\"()\\-/:; ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    # trimming length politely at word boundary\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len]\n",
        "        s = re.sub(r\"\\s+\\S*$\", \"\", s).rstrip(\" ,;:-\")\n",
        "        s += \"...\"\n",
        "    # basic sentence case if looks shouty/garbled\n",
        "    if s and s == s.upper():\n",
        "        s = s.capitalize()\n",
        "    # ensuring closing punctuation\n",
        "    if s and s[-1] not in \".!?\":\n",
        "        s += \".\"\n",
        "    return s\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_text_ascii(prompt=\"\", n_samples=3, max_new_tokens=90, temperature=0.7, top_k=50):\n",
        "    model.eval()\n",
        "    start = encode(prompt if prompt else \"\\n\").unsqueeze(0).to(device)\n",
        "    outs = []\n",
        "    for _ in range(n_samples):\n",
        "        idx = start.clone()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "            if int(next_id) == stoi.get(\"\\n\", -999):\n",
        "                break\n",
        "        raw = decode_ascii(idx[0].tolist())\n",
        "        outs.append(tidy_caption(raw))\n",
        "    model.train()\n",
        "    return outs\n"
      ],
      "metadata": {
        "id": "_pejf2zfT6U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outs = sample_text_ascii(prompt=\"golden hour by the sea #moody\", n_samples=3, temperature=0.75, top_k=60, max_new_tokens=90)\n",
        "for i, s in enumerate(outs, 1):\n",
        "    print(f\"\\n--- sample {i} ---\\n{s}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwqXJAYTTCtF",
        "outputId": "424f39b5-5aad-41c1-f329-aed1665d2daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- sample 1 ---\n",
            "golden hour by the sea #moody a g PO X ;Y g p K P K P.\n",
            "\n",
            "--- sample 2 ---\n",
            "golden hour by the sea #moody X G O D Y ; P \".\n",
            "\n",
            "--- sample 3 ---\n",
            "golden hour by the sea #moodyg e X a C S r P Xn \".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, random, math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1) allowing only nice printable chars and tidying up\n",
        "SAFE_CHARS = set(\"\\n \" + \"\".join([chr(i) for i in range(33,127)]))\n",
        "ALNUM_PUNCT = re.compile(r\"[^A-Za-z0-9#@&.,!?'\\\"()\\-/:; ]+\")\n",
        "\n",
        "def decode_clean(ids):\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        ch = itos[int(i)]\n",
        "        if ch in SAFE_CHARS:\n",
        "            out.append(ch)\n",
        "        else:\n",
        "            out.append(\" \")\n",
        "    txt = \"\".join(out)\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "def tidy_line(s, max_len=140):\n",
        "    s = s.encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
        "    s = ALNUM_PUNCT.sub(\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len]\n",
        "        s = re.sub(r\"\\s+\\S*$\", \"\", s).rstrip(\" ,;:-\") + \"...\"\n",
        "    if s and s[-1] not in \".!?\":\n",
        "        s += \".\"\n",
        "    # cheap “de-soup”: removing sequences of isolated single capitals\n",
        "    s = re.sub(r\"(?<=\\s)[A-Z](?=\\s)\", \"\", s)\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# 2) generating one sample with stricter settings\n",
        "@torch.no_grad()\n",
        "def sample_once(prompt, max_new_tokens=80, temperature=0.6, top_k=40, stop_on_newline=True):\n",
        "    model.eval()\n",
        "    idx = encode((prompt or \"\\n\")).unsqueeze(0).to(device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "        if stop_on_newline and int(next_id) == stoi.get(\"\\n\", -999):\n",
        "            break\n",
        "    raw = decode_clean(idx[0].tolist())\n",
        "    return tidy_line(raw)\n",
        "\n",
        "# 3) scoring caption for “English-ness”\n",
        "VOWELS = set(\"aeiouAEIOU\")\n",
        "def score_caption(s):\n",
        "    if not s: return -1e9\n",
        "    L = len(s)\n",
        "    words = s.split()\n",
        "    # basics\n",
        "    score = 0.0\n",
        "    if 25 <= L <= 150: score += 1.0\n",
        "    score += min(len(words)/12, 1.0)            # more words up to ~12\n",
        "    # letter fraction\n",
        "    letters = sum(c.isalpha() for c in s)\n",
        "    score += (letters / max(1,L)) * 2.0\n",
        "    # vowel ratio encourages pronounceable text\n",
        "    vowels = sum(c in VOWELS for c in s)\n",
        "    if letters > 0: score += (vowels / letters)\n",
        "    # penalize many digits / weird repeats\n",
        "    digits = sum(c.isdigit() for c in s)\n",
        "    score -= 0.5 * (digits / max(1,L))\n",
        "    if re.search(r\"(.)\\1\\1\\1\", s): score -= 1.0\n",
        "    # light bonus for hashtags/mentions but not too many\n",
        "    tags = len(re.findall(r\"(#\\w+|@\\w+)\", s))\n",
        "    score += min(tags, 2) * 0.2\n",
        "    if tags > 4: score -= 0.5\n",
        "    # penalize lots of single-letter tokens\n",
        "    singles = sum(len(w)==1 for w in words)\n",
        "    score -= singles * 0.1\n",
        "    return score\n",
        "\n",
        "# 4) Best-of-N sampler\n",
        "def generate_best_caption(desc=\"\", vibe=\"aesthetic\", trials=60):\n",
        "    style_hint = f\" #{vibe.replace(' ','')}\" if vibe else \"\"\n",
        "    prompt = (desc.strip() + style_hint).strip()\n",
        "    cands = []\n",
        "    # small jitter over temperature/top_k to avoid mode collapse\n",
        "    for _ in range(trials):\n",
        "        t = random.choice([0.55, 0.6, 0.65, 0.7])\n",
        "        k = random.choice([30, 40, 50, 60])\n",
        "        s = sample_once(prompt, temperature=t, top_k=k)\n",
        "        cands.append((score_caption(s), s))\n",
        "    cands.sort(reverse=True)\n",
        "    # return top 3 for safety\n",
        "    return [s for _, s in cands[:3]]"
      ],
      "metadata": {
        "id": "E9KtmbY7T3b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn.functional as F, re\n",
        "from collections import Counter\n",
        "\n",
        "def sample_smart(\n",
        "    prompt=\"\",\n",
        "    max_new_tokens=48,         # shorter = punchier\n",
        "    temperature=0.7,           # calmer wording\n",
        "    top_k=120,                 # allow more choices\n",
        "    top_p=0.90,                # nucleus filter\n",
        "    rep_penalty=1.10,          # nudge away from repetition\n",
        "    stop_on_punct=True,        # stop after ., !, ?\n",
        "    n=8,                       # generate many, we'll pick best\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    start = encode((prompt or \"\\n\")).unsqueeze(0).to(device)\n",
        "    outs = []\n",
        "    for _ in range(n):\n",
        "        idx = start.clone()\n",
        "        seen = Counter()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "\n",
        "            # repetition penalty (simple token-level)\n",
        "            if rep_penalty != 1.0:\n",
        "                for t, c in seen.items():\n",
        "                    if c > 0:\n",
        "                        logits[..., t] /= rep_penalty\n",
        "\n",
        "            # top-k\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
        "            # top-p (nucleus)\n",
        "            if top_p is not None:\n",
        "                sort_logits, sort_idx = torch.sort(logits, descending=True)\n",
        "                probs = F.softmax(sort_logits, dim=-1)\n",
        "                cdf = torch.cumsum(probs, dim=-1)\n",
        "                mask = cdf > top_p\n",
        "                mask[..., 1:] = mask[..., :-1].clone()\n",
        "                mask[..., 0] = False\n",
        "                sort_logits[mask] = -float(\"inf\")\n",
        "                logits = torch.zeros_like(logits).scatter_(-1, sort_idx, sort_logits)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            seen[int(next_id)] += 1\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "            if stop_on_punct:\n",
        "                ch = itos[int(next_id)]\n",
        "                if ch in [\".\", \"!\", \"?\"] or ch == \"\\n\":\n",
        "                    break\n",
        "        outs.append(decode(idx[0].tolist()))\n",
        "    return outs\n"
      ],
      "metadata": {
        "id": "oEr8eh9PX6Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def tidy_line(s, max_len=140):\n",
        "    s = s.encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"[^A-Za-z0-9#@&.,!?'\\\"()\\-/:; ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len]\n",
        "        s = re.sub(r\"\\s+\\S*$\", \"\", s).rstrip(\" ,;:-\")\n",
        "    return s\n",
        "\n",
        "ascii_text = \"\\n\".join(tidy_line(ln) for ln in open(\"data/train.txt\", encoding=\"utf-8\"))\n",
        "open(\"data/train_ascii.txt\", \"w\", encoding=\"utf-8\").write(ascii_text)\n",
        "train_ids = encode(ascii_text + \"\\n\").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9,0.95), weight_decay=0.01)\n",
        "for step in range(1, 401):  # 400 quick steps\n",
        "    ix = torch.randint(0, len(train_ids)-block_size-1, (batch_size,))\n",
        "    xb = torch.stack([train_ids[i:i+block_size] for i in ix]).to(device)\n",
        "    yb = torch.stack([train_ids[i+1:i+1+block_size] for i in ix]).to(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True); loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
        "    if step % 100 == 0: print(f\"polish step {step} | loss {loss.item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSbpqmbxYFgx",
        "outputId": "a5c3cc48-a9a3-476e-ace1-6325f3c16034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polish step 100 | loss 2.202\n",
            "polish step 200 | loss 2.112\n",
            "polish step 300 | loss 2.086\n",
            "polish step 400 | loss 2.028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_caption(s: str):\n",
        "    s = s.split(\"\\n\")[-1]\n",
        "    s = re.sub(r\"[^ -~]+\", \" \", s)              # ASCII only\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    # keeping at most 2 hashtags; drop @handles\n",
        "    parts = s.split()\n",
        "    kept, tags = [], []\n",
        "    for w in parts:\n",
        "        if w.startswith(\"@\"):\n",
        "            continue\n",
        "        if w.startswith(\"#\"):\n",
        "            if len(tags) < 2 and 2 <= len(w) <= 24:\n",
        "                tags.append(w.lower())\n",
        "            continue\n",
        "        kept.append(w)\n",
        "    core = \" \".join(kept).strip(\" ,.;:-\")\n",
        "    tail = (\" \" + \" \".join(tags)) if tags else \"\"\n",
        "    s = (core + tail).strip()\n",
        "    # sentence-case & length sweet-spot\n",
        "    if s and s[0].islower(): s = s[0].upper() + s[1:]\n",
        "    return s[:140]\n",
        "\n",
        "def score_caption(s: str):\n",
        "    # simple heuristic: preferring 6–16 words, no weird repeats\n",
        "    words = s.split()\n",
        "    n = len(words)\n",
        "    length_score = -abs(n - 10)                  # peak at ~10 words\n",
        "    repeat_pen = -sum(c > 2 for c in Counter(words).values())\n",
        "    has_emoji_pen = -1 if re.search(r\"[^\\x20-\\x7E]\", s) else 0\n",
        "    hashtag_bonus = min(2, s.count(\"#\")) * 0.5   # a little stylistic bump\n",
        "    end_punct = 0.5 if s.endswith((\".\", \"!\", \"?\")) else 0\n",
        "    return length_score + repeat_pen + hashtag_bonus + end_punct + has_emoji_pen\n",
        "\n",
        "def generate_best(prompt, vibe=\"moody\"):\n",
        "    raw = sample_smart(f\"{prompt} #{vibe}\", n=12, max_new_tokens=48, temperature=0.7, top_k=120, top_p=0.9)\n",
        "    cleaned = [clean_caption(r) for r in raw]\n",
        "    cleaned = [c for c in cleaned if len(c.split()) >= 5]  # drop ultra-short\n",
        "    if not cleaned:\n",
        "        cleaned = [clean_caption(raw[-1])]\n",
        "    best = max(cleaned, key=score_caption)\n",
        "    return best, cleaned\n",
        "\n",
        "best, alts = generate_best(\"golden hour by the sea\", vibe=\"moody\")\n",
        "print(\"BEST:\", best)\n",
        "print(\"\\nALTS:\")\n",
        "for a in alts[:5]: print(\"-\", a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbTXD-UUZRTo",
        "outputId": "59a485a8-4bc4-45d2-9a84-809cd6899be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST: Golden hour by the sea and with your #moodymenthats #colorcation\n",
            "\n",
            "ALTS:\n",
            "- Golden hour by the sea to favorite part weeks your conted belight b #moodyles\n",
            "- Golden hour by the sea and with your #moodymenthats #colorcation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_caption(text):\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    text = re.sub(r\"[^A-Za-z0-9# ,.!?]\", \"\", text)\n",
        "\n",
        "    text = re.sub(r\"(#\\w+)\\w{3,}\", r\"\\1\", text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "a3tZO8RQaLOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vibe in [\"moody\",\"minimal\",\"playful\",\"wanderlust\"]:\n",
        "    best, _ = generate_best(\"golden hour by the sea\", vibe=vibe)\n",
        "    print(vibe, \"→\", best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zIkfMYeYodm",
        "outputId": "373c586f-279f-485c-8cf0-f6492d207086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moody → golden hour by the sea #mood #intam #Colo #tourth #and\n",
            "minimal → golden hour by the sea #minimalarit #GTAPELon #Loushic #war #sho\n",
            "playful → golden hour by the sea #play #cupposiciala #byarch #encuse\n",
            "wanderlust → golden hour by the sea #wanderlust #berchalfor #mighta #consurelfi #cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best = generate_best_caption(\"golden hour by the sea\", vibe=\"moody\", trials=60)\n",
        "for i,s in enumerate(best,1):\n",
        "    print(f\"\\n>>> {i}. {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qei6kFDUe3H",
        "outputId": "9cd48f32-6b5b-4e81-f635-6c261b8fb8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> 1. golden hour by the sea #moodyou ther to watets #andourn #hestonemarans and bexcke and and in with theatilleng\n",
            "\n",
            ">>> 2. golden hour by the sea #moodyese this per ford and to the ever to @hielewartars ited whork by sery\n",
            "\n",
            ">>> 3. golden hour by the sea #moodyss #consicae are and a for by shore the the sell of the taley trerict bencle com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tidy_line(s, max_len=160):\n",
        "    s = s.encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"[^A-Za-z0-9#@&.,!?'\\\"()\\-/:; ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len]\n",
        "        s = re.sub(r\"\\s+\\S*$\", \"\", s).rstrip(\" ,;:-\")\n",
        "    return s"
      ],
      "metadata": {
        "id": "k9G9VvjFWvKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick polish finetune (optional)\n",
        "polish_text = \"\\n\".join(\n",
        "  tidy_line(re.sub(r\"[^\\x20-\\x7E]\", \" \", ln)) for ln in open(\"data/train.txt\", encoding=\"utf-8\")\n",
        ")\n",
        "open(\"data/train_ascii.txt\", \"w\", encoding=\"utf-8\").write(polish_text)\n",
        "train_ids = encode(polish_text + \"\\n\")\n",
        "\n",
        "# 500 short steps, small LR\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9,0.95), weight_decay=0.01)\n",
        "for step in range(1, 501):\n",
        "    ix = torch.randint(0, len(train_ids)-block_size-1, (batch_size,))\n",
        "    xb = torch.stack([train_ids[i:i+block_size] for i in ix]).to(device)\n",
        "    yb = torch.stack([train_ids[i+1:i+1+block_size] for i in ix]).to(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True); loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
        "    if step % 100 == 0:\n",
        "        print(f\"polish step {step} | loss {loss.item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLLUvrnMWyxg",
        "outputId": "2a348f03-1a87-46cd-ea55-13db1468abd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polish step 100 | loss 2.762\n",
            "polish step 200 | loss 2.580\n",
            "polish step 300 | loss 2.520\n",
            "polish step 400 | loss 2.422\n",
            "polish step 500 | loss 2.339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU3sTnT_aZOt"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_samples(prompt=\"\", n=3, max_new_tokens=160, temperature=0.7, top_k=50):\n",
        "    model.eval()\n",
        "    out = []\n",
        "    start = encode(prompt if prompt else \"\\n\").unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n):\n",
        "            idx = start.clone()\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:]\n",
        "                logits, _ = model(idx_cond)\n",
        "                logits = logits[:, -1, :]\n",
        "                logits = logits / max(temperature, 1e-8)\n",
        "\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = -float('inf')\n",
        "\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat([idx, next_id], dim=1)\n",
        "                if int(next_id) == stoi.get(\"\\n\", -999):\n",
        "                    break\n",
        "            out.append(decode(idx[0].tolist()))\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi7XyFkwaeEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0941a556-12d3-460f-b145-471c7e5a4809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   200 | train 2.022 | val 2.060 | elapsed 0.3 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step   400 | train 1.760 | val 1.867 | elapsed 0.5 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Chit sube today to estay this with a bag team into some boath its and the essent of the some in the turnals.\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "📍 - - - - - - - - medischedds and can seen get the some and lighting as proletections weekend we had those for the teach to of the house, but in the cuts and th\n",
            "step   600 | train 2.091 | val 2.186 | elapsed 0.8 min\n",
            "step   800 | train 2.029 | val 1.987 | elapsed 1.1 min\n",
            "step  1000 | train 1.921 | val 1.998 | elapsed 1.4 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Pass and pressent line in the Skistant Muris Crazara Geography pack to tracture left see the Document remally of our feel the come of the Chook More Happy Today\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The First Conton a time Program and start Sundaino house, the Pass Gally Contorica Day fining Maina a designs of of the look of the world with a trip be not the\n",
            "step  1200 | train 1.941 | val 1.882 | elapsed 1.7 min\n",
            "step  1400 | train 1.851 | val 1.983 | elapsed 1.9 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Mercity features the share little in the film of a few subming is still the at the face of my 1865.00 discover of the projects of the SANGER TPIN DOUS are in th\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "France to me one of the sure of the single recuper for the world. Not exclusive is to his were Best Party as Stay a small many from Consider AIR RW TUMAIN ST is\n",
            "step  1600 | train 1.852 | val 1.901 | elapsed 2.2 min\n",
            "step  1800 | train 1.814 | val 1.857 | elapsed 2.5 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  2000 | train 1.857 | val 1.858 | elapsed 2.8 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Towarden Bari\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "For the Jason world Art Flavor. And it the link in my bio! I have have been think in been bio.\n",
            "\n",
            "step  2200 | train 1.731 | val 1.834 | elapsed 3.1 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  2400 | train 1.752 | val 1.884 | elapsed 3.3 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Have the best family of with my profile. A colorities protection in a conception. I’m not a big which that were were how I was been to success democrafted that \n",
            "\n",
            "--- sample ---\n",
            "\n",
            ".\n",
            "\n",
            "step  2600 | train 1.759 | val 1.799 | elapsed 3.6 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  2800 | train 1.765 | val 1.778 | elapsed 3.9 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  3000 | train 1.659 | val 1.781 | elapsed 4.2 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Food by @natgeo MEETO DIESTAND WATO MIDEAN!!!!! 👇🏽\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The best of a tuxture of the conservation’s that the kids of but I am all of the bring and we wanted to share this recipe of the experience of the special whole\n",
            "step  3200 | train 1.698 | val 1.713 | elapsed 4.4 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  3400 | train 1.695 | val 1.798 | elapsed 4.7 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Making the learn of Norway teams of Monum (@themphotographyraphy @sancemarket_allphotos\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "I want to be the impressed of the power from @pastalia_beach of Fiday and I did the Chilen Borness for the last coast of Kotelbookhood experience your body best\n",
            "step  3600 | train 1.688 | val 1.766 | elapsed 5.0 min\n",
            "step  3800 | train 1.703 | val 1.726 | elapsed 5.3 min\n",
            "step  4000 | train 1.678 | val 1.732 | elapsed 5.5 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "This year again how we are drink to be the source, strll in the journalisms from the most of my life most keynd the fragrange presential and deadling & live of \n",
            "\n",
            "--- sample ---\n",
            "\n",
            ".\n",
            "\n",
            "step  4200 | train 1.684 | val 1.701 | elapsed 5.8 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  4400 | train 1.642 | val 1.701 | elapsed 6.1 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The year of my bio to be a 5 ago, the best of sugar of the first personal in the world in the south of this issue.\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "And the most girls for the Arctive Milk Beautiful Rust is the day of night planet in the At The United Congo to @folloween @vital.style @instandurery @the have \n",
            "step  4600 | train 1.591 | val 1.721 | elapsed 6.4 min\n",
            "step  4800 | train 1.675 | val 1.696 | elapsed 6.7 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  5000 | train 1.665 | val 1.688 | elapsed 6.9 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The New Year Apply February 10 Perfectly family started and visually gets for a lot of the comments that along on the last year to really be able about the prof\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "- 10 years of our world (GT) - - - - - - - - - - - - -\n",
            "\n",
            "step  5200 | train 1.642 | val 1.727 | elapsed 7.2 min\n",
            "step  5400 | train 1.623 | val 1.715 | elapsed 7.5 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The same meals are an amazing being a brown of in how doing up to see your episode of drink in my grow and it’s been way from a totalent of new real content of \n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Sugar change around a past leaf that I'm always been a door and additions— and our hot on my care, and new family grow but I was so always looking to share your\n",
            "step  5600 | train 1.601 | val 1.661 | elapsed 7.8 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  5800 | train 1.565 | val 1.605 | elapsed 8.0 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "step  6000 | train 1.599 | val 1.696 | elapsed 8.3 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Stay in November 2024.\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "@_______________________________________________________________________________________________________________________________________________________________\n",
            "step  6200 | train 1.616 | val 1.640 | elapsed 8.6 min\n",
            "step  6400 | train 1.564 | val 1.665 | elapsed 8.9 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "the same hearts to the name store from @schasilving for the baking of the “Bring” of the amazing from the test connection of the people condition with some of m\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Green and the world of the could change to see these seeds who helps and then we’ve been the started experienced to the next week. They can be again to discover\n",
            "step  6600 | train 1.567 | val 1.649 | elapsed 9.2 min\n",
            "step  6800 | train 1.578 | val 1.699 | elapsed 9.4 min\n",
            "step  7000 | train 1.560 | val 1.598 | elapsed 9.7 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "This that are also can see blend a keep surrounded a dream end, and be every day! @architectures and @rudabeauty of the last adventure and part of the color was\n",
            "\n",
            "--- sample ---\n",
            "\n",
            ".\n",
            "\n",
            "step  7200 | train 1.571 | val 1.659 | elapsed 10.0 min\n",
            "step  7400 | train 1.560 | val 1.682 | elapsed 10.3 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Don’t miss to share this time to catch.\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "This morning for fitness that I want to record the Monday (Ere garlic so much more but I definitely as it?) What are the best beautiful post of the last beautif\n",
            "step  7600 | train 1.547 | val 1.715 | elapsed 10.5 min\n",
            "step  7800 | train 1.520 | val 1.629 | elapsed 10.8 min\n",
            "step  8000 | train 1.557 | val 1.619 | elapsed 11.1 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Happy Week Self-Sunday Day and i am a glacier about my kids in the company days of the community and deserts of finding my life in the windows. Immersions— I kn\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Photography @homestefully in the past of River Earth back, Leica and Chicago, I put that all the maters in families in the Gabrian community and I returned it h\n",
            "step  8200 | train 1.543 | val 1.644 | elapsed 11.4 min\n",
            "step  8400 | train 1.496 | val 1.641 | elapsed 11.6 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "How I got meant to be sharing the year of giving a true to share the left light in Very Colour Pieces and friends the world.  It’s not been sending me a family \n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Art in February 2024 at @sansalamashi in La Marina and @runnyelly the next word to learn more. This is to see your favorite pieces link in my bio to be a profil\n",
            "step  8600 | train 1.581 | val 1.633 | elapsed 11.9 min\n",
            "step  8800 | train 1.559 | val 1.636 | elapsed 12.2 min\n",
            "step  9000 | train 1.621 | val 1.646 | elapsed 12.5 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "My story of the link in my bio to be holiday to travel and facility to share the health of 100 years.  The beauty of Buying who we truly close to watch the best\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Here are also encountry 1 to 2024 days of this recipe for you all know.\n",
            "\n",
            "step  9200 | train 1.490 | val 1.671 | elapsed 12.8 min\n",
            "step  9400 | train 1.531 | val 1.636 | elapsed 13.0 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "More with your many hours of the amazing exclusive @cherrichixxi to make it. The areas an introducing share without my camera part and great lives through the p\n",
            "\n",
            "--- sample ---\n",
            "\n",
            ".\n",
            "\n",
            "step  9600 | train 1.503 | val 1.639 | elapsed 13.3 min\n",
            "step  9800 | train 1.515 | val 1.649 | elapsed 13.6 min\n",
            "step 10000 | train 1.539 | val 1.523 | elapsed 13.9 min\n",
            "  ↳ new best! saved to ckpts/moodmuse_best.pt\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Same behind the autumn with the Antarctica, the recipe of the @tomjournalistics and @goodlevershop to be all in the streets starts. At the end chance healthy be\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "As you want to be this year it was to live a new when my body hair for an end of some profound to tag a bit of that come on a second super waves to share the si\n",
            "step 10200 | train 1.524 | val 1.604 | elapsed 14.2 min\n",
            "step 10400 | train 1.524 | val 1.664 | elapsed 14.4 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            ".\n",
            "\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "It was looking for this #conflict change to photographer this covered in the team. The busy is just like a baby brand and project are really free thrilled to fe\n",
            "step 10600 | train 1.455 | val 1.661 | elapsed 14.7 min\n",
            "step 10800 | train 1.486 | val 1.624 | elapsed 15.0 min\n",
            "step 11000 | train 1.525 | val 1.672 | elapsed 15.3 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Who we had to my favorite every continue with the card, for a long for those salad, and where we are always beyond able to the sea things to move your dreams to\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The creative contact before all the best conflict. The company times we will get to create climbing a south of conversation in the routine of the last time. It \n",
            "step 11200 | train 1.510 | val 1.611 | elapsed 15.5 min\n",
            "step 11400 | train 1.532 | val 1.595 | elapsed 15.8 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "The complication projects. Climbing a slamdance to protect all for photographer, and it’s grateful for a completely we really see the rhythm of the sand cover o\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "It’s the link in my bio to go to meet the beautiful day with the new years to give your feeling. You’re a challenge down my project part of the magic black of a\n",
            "step 11600 | train 1.501 | val 1.577 | elapsed 16.1 min\n",
            "step 11800 | train 1.506 | val 1.628 | elapsed 16.4 min\n",
            "step 12000 | train 1.512 | val 1.601 | elapsed 16.7 min\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Is a picture to into the most beauty of the retreat of the places and now you need to see my stories around the whole of Natgeo Gallery. When I was absolutely a\n",
            "\n",
            "--- sample ---\n",
            "\n",
            "Hahahh\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import math, time, os\n",
        "\n",
        "max_steps = 12000\n",
        "log_every = 200\n",
        "sample_every = 500\n",
        "grad_clip = 1.0\n",
        "best_val = float('inf')\n",
        "ckpt_dir = \"ckpts\"; os.makedirs(ckpt_dir, exist_ok=True)\n",
        "ckpt_path_best = os.path.join(ckpt_dir, \"moodmuse_best.pt\")\n",
        "\n",
        "# simple cosine LR decay (keeps your current optimizer)\n",
        "use_cosine_decay = True\n",
        "base_lr = 3e-4\n",
        "min_lr = 3e-5\n",
        "\n",
        "def set_lr(step):\n",
        "    if not use_cosine_decay:\n",
        "        return\n",
        "    t = step / max(1, max_steps)\n",
        "    lr = min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi * t))\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr\n",
        "\n",
        "t0 = time.time()\n",
        "for step in range(1, max_steps+1):\n",
        "    set_lr(step)\n",
        "\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    # quick val check\n",
        "    if step % log_every == 0:\n",
        "        with torch.no_grad():\n",
        "            xb_val, yb_val = get_batch(\"val\")\n",
        "            _, vloss = model(xb_val, yb_val)\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"step {step:5d} | train {loss.item():.3f} | val {vloss.item():.3f} | elapsed {elapsed/60:.1f} min\")\n",
        "        if vloss.item() < best_val:\n",
        "            best_val = vloss.item()\n",
        "            torch.save({\"model\": model.state_dict(),\n",
        "                        \"itos\": itos, \"stoi\": stoi,\n",
        "                        \"block_size\": block_size},\n",
        "                       ckpt_path_best)\n",
        "            print(f\"  ↳ new best! saved to {ckpt_path_best}\")\n",
        "\n",
        "    if step % sample_every == 0:\n",
        "        for s in generate_samples(\"\", n=2, temperature=0.7, top_k=50):\n",
        "            print(\"\\n--- sample ---\\n\" + s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZMtGXfecvhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d351c7c-b537-4096-dddc-1a0edef900a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- sample ---\n",
            " \n",
            "Both @junellationess.h They are diving for the end of countryside of Barneta California and make up to my heart work in the world.\n",
            "\n",
            "\n",
            "--- sample ---\n",
            " \n",
            "When your submerged past 6 years old with your first special second and culture on your life now you asked around your favorite greats & to a time biggest way t\n",
            "\n",
            "--- sample ---\n",
            " \n",
            "1. @elisoperience\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ckpt = torch.load(\"ckpts/moodmuse_best.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.eval()\n",
        "\n",
        "for s in generate_samples(\"\", n=3, temperature=0.7, top_k=50):\n",
        "    print(\"\\n--- sample ---\\n\", s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "print(\"exists?\", os.path.exists(\"ckpts/moodmuse_best.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUXjRr84FFSZ",
        "outputId": "acb1afdd-4173-4475-9e64-51c93b7790c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the entire model to device\n",
        "from google.colab import files\n",
        "files.download(\"ckpts/moodmuse_best.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GTLmYuU_Gh0t",
        "outputId": "ab3a1aea-0b47-432c-cbfb-9f670c4c8db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_03d81cc7-0553-4c38-95e7-c31f7622e815\", \"moodmuse_best.pt\", 15433562)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile inference.py\n",
        "# inference.py — compat loader for trained checkpoint\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def top_k_filter(logits, k=None):\n",
        "    if k is None:\n",
        "        return logits\n",
        "    k = min(k, logits.size(-1))\n",
        "    v, _ = torch.topk(logits, k)\n",
        "    logits[logits < v[..., [-1]]] = -float(\"inf\")\n",
        "    return logits\n",
        "\n",
        "# ---------- Compat Model ----------\n",
        "class CompatSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # fused qkv like in training code: weight shape [3*n_embd, n_embd]\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "        # causal mask buffer (T, T) — 2D so it broadcasts cleanly to (B,T,T)\n",
        "        mask = torch.tril(torch.ones(block_size, block_size, dtype=torch.bool))\n",
        "        self.register_buffer(\"mask\", mask, persistent=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Robustness: accept (B,1,T,C) by squeezing the singleton\n",
        "        if x.dim() == 4 and x.size(1) == 1:\n",
        "            x = x[:, 0]  # (B, T, C)\n",
        "        if x.dim() != 3:\n",
        "            raise ValueError(f\"CompatSelfAttention expected (B,T,C); got {tuple(x.shape)}\")\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)                         # (B, T, 3C)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)            # each (B, T, C)\n",
        "\n",
        "        # scaled dot-prod attention (single-head to match checkpoint)\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(C)      # (B, T, T)\n",
        "        att = att.masked_fill(~self.mask[:T, :T], float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v                                         # (B, T, C)\n",
        "        y = self.proj(y)\n",
        "        y = self.proj_drop(y)\n",
        "        return y\n",
        "\n",
        "class CompatMLP(nn.Module):\n",
        "    def __init__(self, n_embd, expansion=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, expansion * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(expansion * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class CompatBlock(nn.Module):\n",
        "    def __init__(self, n_embd, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CompatSelfAttention(n_embd, block_size, dropout=dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = CompatMLP(n_embd, expansion=4, dropout=dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class CompatGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_layer, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        # positional as (1, T, C) broadcast — implemented via nn.Embedding on index 0..T-1\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            CompatBlock(n_embd, block_size, dropout=dropout) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        # checkpoint has lm_head.weight only (no bias)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        if T > self.block_size:\n",
        "            idx = idx[:, -self.block_size:]\n",
        "            T = idx.size(1)\n",
        "        pos = torch.arange(T, device=idx.device)               # (T,)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]  # (B, T, C)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "# ---------- Loading checkpoint & tokenizer ----------\n",
        "def load_checkpoint(ckpt_path, device):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # stoi/itos & block_size saved by training loop\n",
        "    stoi = ckpt.get(\"stoi\"); itos = ckpt.get(\"itos\")\n",
        "    block_size = ckpt.get(\"block_size\", 128)\n",
        "\n",
        "    # infer sizes from state_dict\n",
        "    sd = ckpt[\"model\"]\n",
        "    vocab_size, n_embd = sd[\"tok_emb.weight\"].shape\n",
        "    n_layer = max(int(k.split(\".\")[1]) for k in sd.keys() if k.startswith(\"blocks.\")) + 1\n",
        "\n",
        "    model = CompatGPT(vocab_size=vocab_size, n_embd=n_embd, n_layer=n_layer, block_size=block_size)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    model.to(device).eval()\n",
        "    return model, stoi, itos, block_size\n",
        "\n",
        "def encode(text, stoi):\n",
        "    return torch.tensor([stoi[c] for c in text if c in stoi], dtype=torch.long)\n",
        "\n",
        "def decode(ids, itos):\n",
        "    return \"\".join(itos[int(i)] for i in ids)\n",
        "\n",
        "# ---------- Generation ----------\n",
        "@torch.no_grad()\n",
        "def generate_caption(model, stoi, itos, block_size, desc=\"\", vibe=\"aesthetic\",\n",
        "                     max_new_tokens=120, temperature=0.9, top_k=50, device=\"cpu\"):\n",
        "    # style nudger\n",
        "    style_hint = f\" #{vibe.replace(' ', '')}\" if vibe else \"\"\n",
        "    prompt = (desc.strip() + style_hint).strip() or \"\\n\"\n",
        "\n",
        "    idx = encode(prompt, stoi)[None, :].to(device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "        logits = top_k_filter(logits, top_k)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "        # stop early on newline for caption length vibe\n",
        "        if int(next_id) == stoi.get(\"\\n\", -999):\n",
        "            break\n",
        "    return decode(idx[0], itos)\n",
        "\n",
        "# ---------- CLI ----------\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--ckpt\", type=str, default=\"ckpts/moodmuse_best.pt\")\n",
        "    p.add_argument(\"--desc\", type=str, default=\"golden hour on the beach\")\n",
        "    p.add_argument(\"--vibe\", type=str, default=\"moody\")\n",
        "    p.add_argument(\"--max_new_tokens\", type=int, default=120)\n",
        "    p.add_argument(\"--temperature\", type=float, default=0.9)\n",
        "    p.add_argument(\"--top_k\", type=int, default=60)\n",
        "    args = p.parse_args()\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if not os.path.exists(args.ckpt):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {args.ckpt}\")\n",
        "\n",
        "    model, stoi, itos, block_size = load_checkpoint(args.ckpt, device)\n",
        "    caption = generate_caption(\n",
        "        model, stoi, itos, block_size,\n",
        "        desc=args.desc, vibe=args.vibe,\n",
        "        max_new_tokens=args.max_new_tokens,\n",
        "        temperature=args.temperature,\n",
        "        top_k=args.top_k,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"\\n=== Caption ===\")\n",
        "    print(caption)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "JkIFw6v72rVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc761a0-dab2-4b7f-c1ae-1e1c0b37570f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing inference.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "id": "3T0-ixh23UWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0529d003-efd4-486a-e7ba-aff0ef1e013f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 48K\n",
            "drwxr-xr-x 2 root root 4.0K Sep  2 06:50 ckpts\n",
            "drwxr-xr-x 2 root root 4.0K Sep  2 06:49 data\n",
            "drwx------ 5 root root 4.0K Sep  2 07:04 drive\n",
            "-rw-r--r-- 1 root root 8.3K Sep  2 07:04 inference.py\n",
            "-rw-r--r-- 1 root root 7.9K Sep  2 06:29 inference_quick.py\n",
            "-rw-r--r-- 1 root root   69 Sep  2 06:48 kaggle.json\n",
            "drwxr-xr-x 5 root root 4.0K Sep  2 06:47 moodmuse\n",
            "drwxr-xr-x 5 root root 4.0K Sep  2 06:47 MoodMuse\n",
            "drwxr-xr-x 1 root root 4.0K Aug 28 13:43 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpt.py\n",
        "from dataclasses import dataclass\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int\n",
        "    block_size: int\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.1\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # attention scores\n",
        "        wei = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v     # (B,T,hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size, dropout) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.sa  = MultiHeadAttention(n_embd, n_head, block_size, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.ModuleList([Block(config.n_embd, config.n_head, config.block_size, config.dropout) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)                      # (B,T,C)\n",
        "        pos = torch.arange(0, T, device=idx.device, dtype=torch.long)\n",
        "        pos_emb = self.position_embedding_table(pos)[None, :, :]       # (1,T,C)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                                       # (B,T,vocab)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, stop_id=None):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "            if stop_id is not None and int(next_id) == int(stop_id):\n",
        "                break\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "6x8Z95fr48V7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af46f82c-6b25-4f26-d3d1-5e1fead1c643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpt.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ckpts"
      ],
      "metadata": {
        "id": "gYqpwP8y54bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6060174-aba5-40f2-fef1-0445a66d8e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moodmuse_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --ckpt ckpts/moodmuse_best.pt --desc \"sunset over the city skyline\" --vibe moody"
      ],
      "metadata": {
        "id": "Ucn7DHlx576s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74625154-188c-4d8c-a6e4-dab95ca0d708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/inference.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"/content/inference.py\", line 169, in main\n",
            "    model, stoi, itos, block_size = load_checkpoint(args.ckpt, device)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/inference.py\", line 122, in load_checkpoint\n",
            "    model.load_state_dict(sd, strict=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for CompatGPT:\n",
            "\tsize mismatch for blocks.0.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.1.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.2.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.3.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --ckpt ckpts/moodmuse_best.pt \\\n",
        "  --desc \"sunset over the city skyline\" \\\n",
        "  --vibe \"minimal chic\""
      ],
      "metadata": {
        "id": "F_Dhto-ABUYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c815c1b9-5eb7-43f0-8200-a9a17337514a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/inference.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"/content/inference.py\", line 169, in main\n",
            "    model, stoi, itos, block_size = load_checkpoint(args.ckpt, device)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/inference.py\", line 122, in load_checkpoint\n",
            "    model.load_state_dict(sd, strict=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for CompatGPT:\n",
            "\tsize mismatch for blocks.0.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.1.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.2.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.3.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a bit cooler + narrower sampling\n",
        "!python inference.py --ckpt ckpts/moodmuse_best.pt \\\n",
        "  --desc \"sunset over the city skyline\" --vibe \"moody\" \\\n",
        "  --temperature 0.8 --top_k 40 --max_new_tokens 60\n",
        "\n",
        "# warmer, broader, 3 candidates (just run 3 times for now)\n",
        "!python inference.py --ckpt ckpts/moodmuse_best.pt \\\n",
        "  --desc \"sunset over the city skyline\" --vibe \"minimal chic\" \\\n",
        "  --temperature 0.95 --top_k 80 --max_new_tokens 80\n"
      ],
      "metadata": {
        "id": "F5wjNkIZKH9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4c7067-bf7e-45e4-8336-bd048500940a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/inference.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"/content/inference.py\", line 169, in main\n",
            "    model, stoi, itos, block_size = load_checkpoint(args.ckpt, device)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/inference.py\", line 122, in load_checkpoint\n",
            "    model.load_state_dict(sd, strict=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for CompatGPT:\n",
            "\tsize mismatch for blocks.0.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.1.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.2.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.3.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/inference.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"/content/inference.py\", line 169, in main\n",
            "    model, stoi, itos, block_size = load_checkpoint(args.ckpt, device)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/inference.py\", line 122, in load_checkpoint\n",
            "    model.load_state_dict(sd, strict=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for CompatGPT:\n",
            "\tsize mismatch for blocks.0.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.1.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.2.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.3.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --ckpt ckpts/moodmuse_best.pt --desc \"golden hour on the beach\" --vibe \"soft grunge\""
      ],
      "metadata": {
        "id": "xuXnkfBGPLLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421b2404-c445-4929-81f5-f5b8c29359a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/inference.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"/content/inference.py\", line 169, in main\n",
            "    model, stoi, itos, block_size = load_checkpoint(args.ckpt, device)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/inference.py\", line 122, in load_checkpoint\n",
            "    model.load_state_dict(sd, strict=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for CompatGPT:\n",
            "\tsize mismatch for blocks.0.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.1.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.2.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
            "\tsize mismatch for blocks.3.attn.mask: copying a param with shape torch.Size([1, 1, 128, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) making folders\n",
        "!mkdir -p MoodMuse/{src,data/raw,data/processed,outputs/checkpoints,outputs/samples}\n",
        "\n",
        "# 2) keeping empty dirs tracked by git later\n",
        "!touch MoodMuse/data/.gitkeep\n",
        "!touch MoodMuse/outputs/.gitkeep\n",
        "\n",
        "# 3) moving into the project folder for the rest of the session\n",
        "!cd MoodMuse && pwd && ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2mVvT-3ENO3",
        "outputId": "dd513b01-cb4e-4ae2-822f-81a03afba510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MoodMuse\n",
            "total 20\n",
            "drwxr-xr-x 5 root root 4096 Sep  2 16:00 .\n",
            "drwxr-xr-x 1 root root 4096 Sep  2 16:29 ..\n",
            "drwxr-xr-x 4 root root 4096 Sep  2 16:33 data\n",
            "drwxr-xr-x 4 root root 4096 Sep  2 16:33 outputs\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 16:00 src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, json\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- load tokenizer ---\n",
        "tok = json.loads(Path(\"data/tokenizer_char.json\").read_text(encoding=\"utf-8\"))\n",
        "stoi = {k: int(v) for k, v in tok[\"stoi\"].items()}\n",
        "itos = {int(k): v for k, v in tok[\"itos\"].items()}\n",
        "def encode(s): return torch.tensor([stoi[c] for c in s if c in stoi], dtype=torch.long)\n",
        "def decode(ids): return \"\".join(itos[int(i)] for i in ids)\n",
        "\n",
        "# --- load model (same class/arch trained) ---\n",
        "from gpt import GPTConfig, GPT\n",
        "vocab_size = len(itos)\n",
        "block_size = 128\n",
        "cfg = GPTConfig(vocab_size=vocab_size, block_size=block_size,\n",
        "                n_layer=4, n_head=4, n_embd=384, dropout=0.2)\n",
        "model = GPT(cfg).to(device)\n",
        "ckpt = torch.load(\"/content/drive/MyDrive/moodmuse_ckpts/moodmuse_best.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_caption(description:str,\n",
        "                     vibe:str=\"aesthetic\",\n",
        "                     max_new_tokens:int=160,\n",
        "                     temperature:float=0.8,\n",
        "                     top_p:float=0.9):\n",
        "    \"\"\"\n",
        "    description: your photo blurb (e.g., 'golden-hour beach, footprints in sand')\n",
        "    vibe: short style tag ('minimal', 'wholesome', 'moody', 'luxury', 'playful', etc.)\n",
        "    \"\"\"\n",
        "    # a tiny prompt that nudges IG-style\n",
        "    prompt = f\"{vibe.title()} vibe — {description}\\n\"\n",
        "    idx = encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
        "        # nucleus sampling\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cumsum > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs = sorted_probs / (sorted_probs.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "        next_id = sorted_idx.gather(-1, torch.multinomial(sorted_probs, num_samples=1))\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "        if int(next_id) == stoi.get(\"\\n\", -999):  # stop at newline\n",
        "            break\n",
        "\n",
        "    raw = decode(idx[0].tolist())\n",
        "    generated = raw[len(prompt):].strip()\n",
        "\n",
        "    # simple post-processing\n",
        "    generated = generated.replace(\"..\", \".\").replace(\"  \", \" \").strip()\n",
        "    # cap length to ~2200 chars\n",
        "    if len(generated) > 350:\n",
        "        generated = generated[:350].rsplit(\" \", 1)[0] + \"…\"\n",
        "    return generated\n",
        "\n",
        "# quick sanity:\n",
        "print(generate_caption(\"golden-hour coffee on a window sill\", vibe=\"minimal\"))\n",
        "print(generate_caption(\"rainy neon street reflection\", vibe=\"moody\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "anbjxS-LRmmL",
        "outputId": "b707f02d-e50a-4b2b-b130-835abad1b596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/moodmuse_ckpts/moodmuse_best.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2488471644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 n_layer=4, n_head=4, n_embd=384, dropout=0.2)  # match your training config\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/moodmuse_ckpts/moodmuse_best.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/moodmuse_ckpts/moodmuse_best.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Demo-polish helpers (drop this in a new cell) ===\n",
        "import re, random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1) Decoding tweaks\n",
        "DECODE_T = 0.7        # lower temp -> fewer weird neologisms\n",
        "DECODE_TOPK = 40      # tighter top-k\n",
        "N_SAMPLES = 8         # generate multiple and pick the cleanest\n",
        "\n",
        "# 2) A small, safe hashtag whitelist you’re happy to show\n",
        "HASHTAG_WHITELIST = [\n",
        "    \"moody\",\"minimal\",\"playful\",\"wanderlust\",\"sunset\",\"vibes\",\"aesthetic\",\n",
        "    \"goldenhour\",\"seaside\",\"cityscape\",\"film\",\"portrait\",\"travel\",\"weekend\",\n",
        "    \"softlight\",\"summer\",\"cozy\",\"streetstyle\"\n",
        "]\n",
        "\n",
        "# 3) helpers\n",
        "def norm_token(t): return re.sub(r\"[^A-Za-z0-9#]\", \"\", t)\n",
        "\n",
        "def pick_whitelist_tag(raw_tag):\n",
        "    \"\"\"Map a fuzzy model tag to your whitelist, else return ''.\"\"\"\n",
        "    t = raw_tag.lower().lstrip(\"#\")\n",
        "    if not t: return \"\"\n",
        "    # exact\n",
        "    if t in HASHTAG_WHITELIST: return f\"#{t}\"\n",
        "    # simple similarity on prefix/containment\n",
        "    best = \"\"\n",
        "    best_score = 0.0\n",
        "    for w in HASHTAG_WHITELIST:\n",
        "        # overlap score: common prefix length / max len\n",
        "        pref = 0\n",
        "        for a,b in zip(t, w):\n",
        "            if a==b: pref += 1\n",
        "            else: break\n",
        "        score = max(pref/len(w), pref/max(1,len(t)))\n",
        "        if t in w or w in t: score = max(score, 0.66)\n",
        "        if score > best_score:\n",
        "            best_score, best = score, w\n",
        "    return f\"#{best}\" if best_score >= 0.5 else \"\"  # require a decent match\n",
        "\n",
        "def clean_caption(text, vibe=None, keep_tags=2):\n",
        "    # keep ASCII, collapse space\n",
        "    text = re.sub(r\"[^\\x20-\\x7E]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # split words & tags\n",
        "    toks = [norm_token(t) for t in text.split()]\n",
        "    words, tags = [], []\n",
        "    for t in toks:\n",
        "        if t.startswith(\"#\"):\n",
        "            m = pick_whitelist_tag(t)\n",
        "            if m and m not in tags:\n",
        "                tags.append(m)\n",
        "        else:\n",
        "            # filter obvious garbage: mega-long or mostly consonants\n",
        "            if len(t) <= 20 and not re.search(r\"[bcdfghjklmnpqrstvwxyz]{5,}\", t.lower()):\n",
        "                words.append(t)\n",
        "\n",
        "    # ensure vibe tag present (mapped to whitelist if possible)\n",
        "    if vibe:\n",
        "        vtag = pick_whitelist_tag(\"#\" + vibe)\n",
        "        if vtag and vtag not in tags:\n",
        "            tags = [vtag] + tags\n",
        "\n",
        "    # keep up to N tags, unique\n",
        "    tags = tags[:keep_tags]\n",
        "\n",
        "    # sentencecase words and limit to 8–14 tokens for IG feel\n",
        "    if not words:\n",
        "        words = [\"Golden\", \"hour\", \"by\", \"the\", \"sea\"]\n",
        "    words = words[:14]\n",
        "    sent = \" \".join(words).strip()\n",
        "    if sent:\n",
        "        sent = sent[0].upper() + sent[1:]\n",
        "        if not re.search(r\"[.!?]$\", sent):\n",
        "            sent += \".\"\n",
        "    return (sent + (\" \" + \" \".join(tags) if tags else \"\")).strip()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_once(model, stoi, itos, block_size, prompt, device=\"cuda\", T=DECODE_T, top_k=DECODE_TOPK, max_new_tokens=64):\n",
        "    # encode\n",
        "    ids = torch.tensor([stoi.get(c, None) for c in prompt if c in stoi], dtype=torch.long)\n",
        "    if ids.numel() == 0:\n",
        "        ids = torch.tensor([stoi.get(\"\\n\")], dtype=torch.long)\n",
        "    ids = ids.unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = ids[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(T, 1e-8)\n",
        "        if top_k is not None:\n",
        "            v,_ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:,[-1]]] = -float(\"inf\")\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        ids = torch.cat([ids, next_id], dim=1)\n",
        "        # optional early stop on newline\n",
        "        if int(next_id) == stoi.get(\"\\n\", -999):\n",
        "            break\n",
        "    return \"\".join(itos[int(i)] for i in ids[0])\n",
        "\n",
        "def generate_best(desc, vibe=\"moody\", tries=N_SAMPLES):\n",
        "    # style hint appended to steer tone\n",
        "    base = (desc.strip() + f\" #{vibe}\".replace(\" \", \"\")).strip()\n",
        "    cands = []\n",
        "    for _ in range(tries):\n",
        "        raw = sample_once(model, stoi, itos, block_size, prompt=base, device=device)\n",
        "        cleaned = clean_caption(raw, vibe=vibe, keep_tags=2)\n",
        "        # score: shorter & more whitelisted tags preferred\n",
        "        tag_count = len(re.findall(r\"#\\w+\", cleaned))\n",
        "        nonsense_pen = 1 if re.search(r\"[bcdfghjklmnpqrstvwxyz]{6,}\", cleaned.lower()) else 0\n",
        "        score = -len(cleaned) - 10*nonsense_pen + 8*tag_count\n",
        "        cands.append((score, cleaned))\n",
        "    cands.sort(reverse=True, key=lambda x: x[0])\n",
        "    return cands[0][1], cands   # best, all\n",
        "\n",
        "# === Run demo for multiple vibes ===\n",
        "test_desc = \"golden hour by the sea\"\n",
        "for v in [\"moody\",\"minimal\",\"playful\",\"wanderlust\"]:\n",
        "    best, _ = generate_best(test_desc, vibe=v)\n",
        "    print(f\"{v:10s} → {best}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoAy6KcRbNfX",
        "outputId": "f9f10e14-570f-42a0-8686-94e673bb5524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moody      → Golden hour by the sea#moody. #moody\n",
            "minimal    → Golden hour by the sea#minimally. #minimal\n",
            "playful    → Golden hour by the sea#playful. #playful\n",
            "wanderlust → Golden hour by the sea#wanderlust. #wanderlust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PATCH: better cleaner + smart second tag ---\n",
        "\n",
        "# simple keyword→tag map for a tasteful second hashtag\n",
        "KEYWORD_TAGS = {\n",
        "    \"golden\": \"goldenhour\",\n",
        "    \"sunset\": \"sunset\",\n",
        "    \"dawn\": \"goldenhour\",\n",
        "    \"dusk\": \"goldenhour\",\n",
        "    \"beach\": \"seaside\",\n",
        "    \"sea\": \"seaside\",\n",
        "    \"ocean\": \"seaside\",\n",
        "    \"city\": \"cityscape\",\n",
        "    \"street\": \"streetstyle\",\n",
        "    \"film\": \"film\",\n",
        "    \"travel\": \"travel\",\n",
        "    \"summer\": \"summer\",\n",
        "    \"cozy\": \"cozy\",\n",
        "}\n",
        "\n",
        "def second_tag_from_desc(desc: str):\n",
        "    d = desc.lower()\n",
        "    for k, tag in KEYWORD_TAGS.items():\n",
        "        if k in d:\n",
        "            return f\"#{tag}\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def clean_caption(text, vibe=None, keep_tags=2, desc_for_extra_tag=\"\"):\n",
        "    # make sure hashtags have a leading space if glued to a word\n",
        "    text = re.sub(r\"([A-Za-z0-9])(#)\", r\"\\1 \\2\", text)\n",
        "\n",
        "    # ASCII only + collapse spaces\n",
        "    text = re.sub(r\"[^\\x20-\\x7E]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # tokenize and normalize\n",
        "    toks = [norm_token(t) for t in text.split()]\n",
        "    words, tags = [], []\n",
        "    for t in toks:\n",
        "        if t.startswith(\"#\"):\n",
        "            m = pick_whitelist_tag(t)\n",
        "            if m and m not in tags:\n",
        "                tags.append(m)\n",
        "        else:\n",
        "            # filter obvious junk\n",
        "            if 1 <= len(t) <= 20 and not re.search(r\"[bcdfghjklmnpqrstvwxyz]{5,}\", t.lower()):\n",
        "                words.append(t)\n",
        "\n",
        "    # ensure vibe tag (once)\n",
        "    if vibe:\n",
        "        vtag = pick_whitelist_tag(\"#\" + vibe)\n",
        "        if vtag:\n",
        "            tags = [t for t in tags if t.lower() != vtag.lower()]  # de-dup any fuzzy version\n",
        "            tags.insert(0, vtag)\n",
        "\n",
        "    # optional second tag from description, if we still have room\n",
        "    if len(tags) < keep_tags and desc_for_extra_tag:\n",
        "        extra = second_tag_from_desc(desc_for_extra_tag)\n",
        "        extra = pick_whitelist_tag(extra) if extra else \"\"\n",
        "        if extra and extra not in tags:\n",
        "            tags.append(extra)\n",
        "\n",
        "    # cap tag count\n",
        "    tags = tags[:keep_tags]\n",
        "\n",
        "    # sentencecase, short & tidy\n",
        "    if not words:\n",
        "        words = [\"Golden\", \"hour\", \"by\", \"the\", \"sea\"]\n",
        "    words = words[:14]\n",
        "    sent = \" \".join(words).strip()\n",
        "    # remove any trailing period directly before a hashtag (we’ll add one properly)\n",
        "    sent = re.sub(r\"\\.(\\s*#)\", r\"\\1\", sent)\n",
        "    if sent:\n",
        "        sent = sent[0].upper() + sent[1:]\n",
        "        if not re.search(r\"[.!?]$\", sent):\n",
        "            sent += \".\"\n",
        "\n",
        "    return (sent + (\" \" + \" \".join(tags) if tags else \"\")).strip()\n",
        "\n",
        "\n",
        "def generate_best(desc, vibe=\"moody\", tries=N_SAMPLES):\n",
        "    base = (desc.strip() + f\" #{vibe}\".replace(\" \", \"\")).strip()\n",
        "    cands = []\n",
        "    for _ in range(tries):\n",
        "        raw = sample_once(model, stoi, itos, block_size, prompt=base, device=device)\n",
        "        cleaned = clean_caption(raw, vibe=vibe, keep_tags=2, desc_for_extra_tag=desc)\n",
        "        # score: shorter, with 2 tags gets a small bonus\n",
        "        tag_count = len(re.findall(r\"#\\w+\", cleaned))\n",
        "        nonsense_pen = 1 if re.search(r\"[bcdfghjklmnpqrstvwxyz]{6,}\", cleaned.lower()) else 0\n",
        "        score = -len(cleaned) - 10*nonsense_pen + 6*tag_count\n",
        "        cands.append((score, cleaned))\n",
        "    cands.sort(reverse=True, key=lambda x: x[0])\n",
        "    return cands[0][1], cands"
      ],
      "metadata": {
        "id": "2YCM3fQ5b4A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for v in [\"moody\",\"minimal\",\"playful\",\"wanderlust\"]:\n",
        "    print(f\"{v:10s} → {generate_best('golden hour by the sea', vibe=v)[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RXw68n0b6dY",
        "outputId": "b5d13bc7-a3ce-4dab-a56f-6727daecd250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moody      → Golden hour by the sea. #moody #goldenhour\n",
            "minimal    → Golden hour by the sea. #minimal #travel\n",
            "playful    → Golden hour by the sea. #playful #sunset\n",
            "wanderlust → Golden hour by the sea. #wanderlust #sunset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# (A) create folders\n",
        "cd /content\n",
        "mkdir -p MoodMuse/{src,data,outputs/ckpts,outputs/samples}\n",
        "\n",
        "# (B) copying working assets into the repo\n",
        "#  - the tokenizer built in the notebook\n",
        "#  - the best checkpoint trained (the one with tok_emb.* keys)\n",
        "cp -v /content/data/tokenizer_char.json /content/MoodMuse/data/\n",
        "cp -v /content/ckpts/moodmuse_best.pt  /content/MoodMuse/outputs/ckpts/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9UtvsJpd-W7",
        "outputId": "b8d450ec-79c8-4d9d-f94e-b84a1ee85a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/data/tokenizer_char.json' -> '/content/MoodMuse/data/tokenizer_char.json'\n",
            "'/content/ckpts/moodmuse_best.pt' -> '/content/MoodMuse/outputs/ckpts/moodmuse_best.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -v /content/inference_quick.py /content/MoodMuse/src/inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "Gz0uKkqveHjO",
        "outputId": "1c79f45d-1447-4c68-f0d4-6d0569ea89f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cp: cannot stat '/content/inference_quick.py': No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'# if your working script is /content/inference_quick.py\\ncp -v /content/inference_quick.py /content/MoodMuse/src/inference.py\\n'' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1286622183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# if your working script is /content/inference_quick.py\\ncp -v /content/inference_quick.py /content/MoodMuse/src/inference.py\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'# if your working script is /content/inference_quick.py\\ncp -v /content/inference_quick.py /content/MoodMuse/src/inference.py\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}